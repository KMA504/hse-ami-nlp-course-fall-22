{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ccN1AlFhNo"
      },
      "source": [
        "## Классификация текста с использованием CNN\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PragmaticsLab/NLP-course-AMI/blob/dev/seminars/sem3_classification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ8_zAI8FhNp"
      },
      "source": [
        "Используя данные отзывов IMDB, построим CNN для классификации документов на позитивный и негативный классы.\n",
        "\n",
        "Источник изложения: https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHKLq9hEFhNr"
      },
      "source": [
        "В предположении, что PyTorch уже установлен, поставим дополнительные модули и загрузим модель для токенизации:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgGxeuQjG9NI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cf8581-7dae-46bf-e9e7-1dbd5a7876fb"
      },
      "source": [
        "#!pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n",
        "#!pip3 install torch torchvision torchaudio\n",
        "#!pip3 install torch==1.8.0 #torchvision==0.9.0 torchaudio==0.8.0\n",
        "!pip3 install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 1.3 MB/s eta 0:14:35tcmalloc: large alloc 1147494400 bytes == 0x39e12000 @  0x7fdcc011c615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x58f49e 0x51837f 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4ba70a 0x538136 0x590055 0x51b180 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x58f49e\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 1.2 MB/s eta 0:12:22tcmalloc: large alloc 1434370048 bytes == 0x7e468000 @  0x7fdcc011c615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x58f49e 0x51837f 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4ba70a 0x538136 0x590055 0x51b180 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x58f49e\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 1.3 MB/s eta 0:08:32tcmalloc: large alloc 1792966656 bytes == 0x329a000 @  0x7fdcc011c615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x58f49e 0x51837f 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4ba70a 0x538136 0x590055 0x51b180 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x58f49e\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.2 MB/s eta 0:04:03tcmalloc: large alloc 2241208320 bytes == 0x6e082000 @  0x7fdcc011c615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x58f49e 0x51837f 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4ba70a 0x538136 0x590055 0x51b180 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x58f49e\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0xf39e4000 @  0x7fdcc011b1e7 0x4b2150 0x4b21dc 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x58f2a7 0x517947 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51837f 0x5b41c5\n",
            "tcmalloc: large alloc 2477817856 bytes == 0x1de106000 @  0x7fdcc011c615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x58f2a7 0x517947 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x4ba899 0x4d29f9\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 5.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kiZro9eG-bG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e5362c-618e-4e60-cabb-4d3e85a952cb"
      },
      "source": [
        "#!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-WRgs-VFhNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4cd3be-ae1d-48e9-88b5-51f6c8792f57"
      },
      "source": [
        "!pip install torchtext==0.9.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 16.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.8.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext==0.9.0) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "Successfully installed torchtext-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB7uZ5L7FhN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8d6e0f-c36e-41dd-8f07-989eefdf6958"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLGeXcUjFhN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd58bebd-1c8b-4a67-cc37-37b0704b4b9d"
      },
      "source": [
        "#!python3.6 -m spacy download en\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaPk16PLmgnw"
      },
      "source": [
        "import spacy\n",
        "#import en\n",
        "en_nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGAJBxfFhN_"
      },
      "source": [
        "Загрузим датасет и получим из него выборку:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsHAXknhFhOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a64d94-74b3-4f56-c4fb-266bda598079"
      },
      "source": [
        "import torch\n",
        "#import torch.cuda\n",
        "#from torchtext import data\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "#from torchtext import datasets\n",
        "#from torchtext.data.utils import get_tokenizer\n",
        "#from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "#tokenizer = get_tokenizer('spacy')\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "train_src, test = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8.0+cu111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:14<00:00, 5.93MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJl0ZOPBElSE"
      },
      "source": [
        "Попробуем обучить простую CNN на векторах слов. С учётом того, что в коллекции 100К уникальных слов, и векторы получатся достаточно громоздкие, урежем коллекцию до 25К слов, для всех прочих заведя токен unk (unknown). Кроме того, разобьём обучающий набор данных на собственно обучающий и валидационный набор для настройки параметров."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nuZQIq1FhOl"
      },
      "source": [
        "#print(len(train_src))\n",
        "import random\n",
        "#train, valid = torch.utils.data.random_split(train_src,[20000, 5000], random.seed(SEED))\n",
        "train, valid = train_src.split(random_state=random.seed(SEED))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsdwfpM5FhOp"
      },
      "source": [
        "#def yield_tokens(data_iter):\n",
        "#    for _, text in data_iter:\n",
        "#        yield tokenizer(text)\n",
        "\n",
        "#vocab = build_vocab_from_iterator(yield_tokens(train_src), specials=[\"<unk>\"])\n",
        "#vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "TEXT.build_vocab(train, max_size=25000)\n",
        "LABEL.build_vocab(train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAizbRUkXMVt"
      },
      "source": [
        "#text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "#label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM9FpclZFhOu"
      },
      "source": [
        "Выше в словаре, помимо отсечения 25К наиболее частых слов, также появятся два специальных токена - unk и pad (padding).\n",
        "\n",
        "Теперь создадим батчи, предварительно отсортировав тексты по длине, чтобы минимизировать вставки pad-токенов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnHqmLZTFhOv"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf2viKyOE5FF"
      },
      "source": [
        "Опишем функцию подсчёта accuracy, а также функции обучения и применения сети:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQo4PuPzFhPE"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(F.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jepSRNlkFhPI"
      },
      "source": [
        "def train_func(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text.cuda()).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions.float(), batch.label.float().cuda())\n",
        "        acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb0KPBl8FhPL"
      },
      "source": [
        "def evaluate_func(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text.cuda()).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions.float(), batch.label.float().cuda())\n",
        "            acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
        "\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm187c4CFhPX"
      },
      "source": [
        "Начнём с подготовки данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By88bjwfFhPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209bd5ff-3115-4f76-9b11-88b9f47b4543"
      },
      "source": [
        "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "\n",
        "BATCH_SIZEBATCH_S  = 8\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.33MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:13<00:00, 29531.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHEDOjrAFhP5"
      },
      "source": [
        "Для создания свёрточного слоя воспользуемся nn.Conv2d, in_channels в нашем случае один (текст), out_channels -- это число фильтров и размер ядер всех фильтров. Каждый фильтр будет иметь размерность [n x размерность эмбеддинга], где n - размер обрабатываемой n-граммы.\n",
        "\n",
        "Важно, чтобы предложения имели длину не меньше размера самого большого из используемых фильтров (здесь это не страшно, поскольку в используемых данных нет текстов, состоящих из пяти и менее слов)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb5g3czaFhP6"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #x = [sent len, batch size]\n",
        "        x = x.permute(1, 0)\n",
        "                \n",
        "        #x = [batch size, sent len]\n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "        return self.fc(cat)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_xPva9zFhP8"
      },
      "source": [
        "Сейчас мы можем использовать только три различных фильтра, хотелось бы больше. Воспользуйтесь nn.ModuleList и перепишите класс сети для того, чтобы фильтров создавалось по количеству элементов в filter_sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG4nFyJaqESM"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eWasooHFhQA"
      },
      "source": [
        "Cоздадим модель и загрузим предобученные эмбеддинги:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q_Yvs_gFhQB"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWlPRIqHFhQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7747ed71-2017-458c-c848-c40a9db4b85e"
      },
      "source": [
        "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.3248,  0.4696,  0.3094,  ..., -0.1104,  0.2602, -0.1512],\n",
              "        [-0.2263,  0.9641, -0.2169,  ..., -0.3284, -0.6441,  0.7686],\n",
              "        [ 0.0885, -0.1141, -0.1055,  ...,  0.1373,  0.2495,  0.3573]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnFgj1oRGl4k"
      },
      "source": [
        "BATCH_SIZE  = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmWIaqIbFhQF"
      },
      "source": [
        "Используя определённые ранее функции, запустим обучение с оптимизатором Adam и оценим качество на валидации и тесте:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6g2pMt9rX2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4a9b90-c1ff-4162-e658-10497fe417f8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 22 09:24:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0    27W /  70W |   1340MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np-BTnydFhQF"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.cuda()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZC7S33pFhQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5809b7-273b-49bf-8089-e08836bbffa6"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train_func(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_func(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Train Loss: 0.505, Train Acc: 73.69%, Val. Loss: 0.340, Val. Acc: 85.54%\n",
            "Epoch: 02, Train Loss: 0.307, Train Acc: 86.99%, Val. Loss: 0.293, Val. Acc: 87.55%\n",
            "Epoch: 03, Train Loss: 0.218, Train Acc: 91.42%, Val. Loss: 0.260, Val. Acc: 89.15%\n",
            "Epoch: 04, Train Loss: 0.146, Train Acc: 94.72%, Val. Loss: 0.272, Val. Acc: 88.81%\n",
            "Epoch: 05, Train Loss: 0.088, Train Acc: 97.02%, Val. Loss: 0.303, Val. Acc: 88.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXQYQCLCFhQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63a9997-da55-4857-d890-4fedabe5093f"
      },
      "source": [
        "test_loss , test_acc = evaluate_func(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.319, Test Acc: 87.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfuamkw28rgp"
      },
      "source": [
        "# Аугментация данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zDRG-jIdNB"
      },
      "source": [
        "В нашем примере данные были сбалансированными. А как работать с несбалансированными данными?\n",
        "\n",
        "Рассмотрим задачу распознавания тональности твитов, взятых из [Twitter Sentimental Analysis challenge](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/).\n",
        "\n",
        "Источник изложения: https://github.com/mabusalah/Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLKzWFu-QWAm"
      },
      "source": [
        "Получим данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_WpbqWhk-a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cd3718-f451-4f08-d8ef-64162ee856f6"
      },
      "source": [
        "!wget https://www.dropbox.com/s/3eefrxvfin11m5p/test_tweets.csv\n",
        "!wget https://www.dropbox.com/s/rzpc5gxlvl6zh8a/train.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-22 08:08:08--  https://www.dropbox.com/s/3eefrxvfin11m5p/test_tweets.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/3eefrxvfin11m5p/test_tweets.csv [following]\n",
            "--2022-09-22 08:08:09--  https://www.dropbox.com/s/raw/3eefrxvfin11m5p/test_tweets.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucc057c8511be622d29f57b28573.dl.dropboxusercontent.com/cd/0/inline/BtYEd_5Pv1QId9DpyGifwRsP65_cEXYiAGq6Eg0ZOxUbxpe0RyC2BIQx-IdmI4PRycvIB8xXuSQ2L5J3xIoA4fkEneCdH8VPFQHWLfEfDznXCX0w9HiMi_y7_kKYXJTTp1igwgOaOWOsrNHHglilQXq2F4wYmQS2ucBYooMVRdL-6Q/file# [following]\n",
            "--2022-09-22 08:08:10--  https://ucc057c8511be622d29f57b28573.dl.dropboxusercontent.com/cd/0/inline/BtYEd_5Pv1QId9DpyGifwRsP65_cEXYiAGq6Eg0ZOxUbxpe0RyC2BIQx-IdmI4PRycvIB8xXuSQ2L5J3xIoA4fkEneCdH8VPFQHWLfEfDznXCX0w9HiMi_y7_kKYXJTTp1igwgOaOWOsrNHHglilQXq2F4wYmQS2ucBYooMVRdL-6Q/file\n",
            "Resolving ucc057c8511be622d29f57b28573.dl.dropboxusercontent.com (ucc057c8511be622d29f57b28573.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to ucc057c8511be622d29f57b28573.dl.dropboxusercontent.com (ucc057c8511be622d29f57b28573.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1635543 (1.6M) [text/plain]\n",
            "Saving to: ‘test_tweets.csv’\n",
            "\n",
            "test_tweets.csv     100%[===================>]   1.56M  7.60MB/s    in 0.2s    \n",
            "\n",
            "2022-09-22 08:08:10 (7.60 MB/s) - ‘test_tweets.csv’ saved [1635543/1635543]\n",
            "\n",
            "--2022-09-22 08:08:10--  https://www.dropbox.com/s/rzpc5gxlvl6zh8a/train.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/rzpc5gxlvl6zh8a/train.csv [following]\n",
            "--2022-09-22 08:08:11--  https://www.dropbox.com/s/raw/rzpc5gxlvl6zh8a/train.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc84ca2b8beaac471c560ba81013.dl.dropboxusercontent.com/cd/0/inline/BtaVyW9SHQ_E5s747WUwLWRKxtpiX_pk_F5WiUGTvbWxU1Yp31X_FQiiMqkgg3oUveEQ3WJ8hQnksX-edjjXYNh2cvkcTCNLJI_E_bKk_n7y8kK-LZBU0yxwXlxaKD-JFKow3VtI3BxWhZdGtwq6Ngx4pGOV85B58p58xQ2-luIJeQ/file# [following]\n",
            "--2022-09-22 08:08:12--  https://uc84ca2b8beaac471c560ba81013.dl.dropboxusercontent.com/cd/0/inline/BtaVyW9SHQ_E5s747WUwLWRKxtpiX_pk_F5WiUGTvbWxU1Yp31X_FQiiMqkgg3oUveEQ3WJ8hQnksX-edjjXYNh2cvkcTCNLJI_E_bKk_n7y8kK-LZBU0yxwXlxaKD-JFKow3VtI3BxWhZdGtwq6Ngx4pGOV85B58p58xQ2-luIJeQ/file\n",
            "Resolving uc84ca2b8beaac471c560ba81013.dl.dropboxusercontent.com (uc84ca2b8beaac471c560ba81013.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc84ca2b8beaac471c560ba81013.dl.dropboxusercontent.com (uc84ca2b8beaac471c560ba81013.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3103165 (3.0M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   2.96M  9.31MB/s    in 0.3s    \n",
            "\n",
            "2022-09-22 08:08:12 (9.31 MB/s) - ‘train.csv’ saved [3103165/3103165]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILRsSOuWJGFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7443103-1b19-4f66-acfc-ba24520da457"
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv('test_tweets.csv')\n",
        "print(\"Test Set:\"% test.columns, test.shape, len(test))\n",
        "train = pd.read_csv('train.csv')\n",
        "print(\"Training Set:\"% train.columns, train.shape, len(train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set: (17197, 2) 17197\n",
            "Training Set: (31962, 3) 31962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBtAsQDNJPXX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "941cd8a2-9d03-4e44-ab03-bb26597e94b7"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  label                                              tweet\n",
              "0   1      0   @user when a father is dysfunctional and is s...\n",
              "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
              "2   3      0                                bihday your majesty\n",
              "3   4      0  #model   i love u take with u all the time in ...\n",
              "4   5      0             factsguide: society now    #motivation"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ba2d19b-7102-4b54-92a4-4673d046eda3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ba2d19b-7102-4b54-92a4-4673d046eda3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ba2d19b-7102-4b54-92a4-4673d046eda3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ba2d19b-7102-4b54-92a4-4673d046eda3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV-4lGQVJRc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "891b59f8-9dcc-49a1-e784-413825395754"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id                                              tweet\n",
              "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
              "1  31964   @user #white #supremacists want everyone to s...\n",
              "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
              "3  31966  is the hp and the cursed child book up for res...\n",
              "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4fbf48a6-2d2d-4c15-83cf-3e27376a6c99\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31963</td>\n",
              "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31964</td>\n",
              "      <td>@user #white #supremacists want everyone to s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31965</td>\n",
              "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31966</td>\n",
              "      <td>is the hp and the cursed child book up for res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31967</td>\n",
              "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4fbf48a6-2d2d-4c15-83cf-3e27376a6c99')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4fbf48a6-2d2d-4c15-83cf-3e27376a6c99 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4fbf48a6-2d2d-4c15-83cf-3e27376a6c99');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xarXsO4DJU8Y"
      },
      "source": [
        "Итак, посмотрим, какой процент от общей выборки занимают позитивные и негативные примеры."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT7Rz3MNJTVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e447f09e-8c1b-4427-ec7b-1f95d8d51829"
      },
      "source": [
        "print(\"Positive: \", train.label.value_counts()[0]/len(train)*100,\"%\")\n",
        "print(\"Negative: \", train.label.value_counts()[1]/len(train)*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive:  92.98542018647143 %\n",
            "Negative:  7.014579813528565 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB6Jaz-mJ1zR"
      },
      "source": [
        "93% vs. 7% - данные определенно несбалансированны, что, в свою очередь, негативно влияет на точность предсказания.\n",
        "Для начала поработаем с исходными данными и оценим точность классификации.\n",
        "Начнем с предобработки данных: уберем из твитов числа, html/xml-тэги, специальные символы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCmZHO1kJfpP"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup #для работы с html/xml-тэгами\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter=PorterStemmer()\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "\n",
        "    words = tok.tokenize(lower_case)\n",
        "    \n",
        "    stem_sentence=[]\n",
        "    for word in words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    words=\"\".join(stem_sentence).strip()\n",
        "    return words\n",
        "\n",
        "nums = [0,len(train)]\n",
        "clean_tweet_texts = []\n",
        "for i in range(nums[0],nums[1]):\n",
        "    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))\n",
        "    \n",
        "nums = [0,len(test)]\n",
        "test_tweet_texts = []\n",
        "\n",
        "for i in range(nums[0],nums[1]):\n",
        "    test_tweet_texts.append(tweet_cleaner(test['tweet'][i])) \n",
        "    \n",
        "train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])\n",
        "train_clean['label'] = train.label\n",
        "train_clean['id'] = train.id\n",
        "test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])\n",
        "test_clean['id'] = test.id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkQ5fDVUM9EU"
      },
      "source": [
        "Разделим данные на обучающие и проверочные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23SW3-tMdKk"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['tweet'],train_clean['label'])\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfj1ZjQANVSK"
      },
      "source": [
        "Рассчитаем TF-IDF признаки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNw9U1WnNKfq"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
        "tfidf_vect.fit(train_clean['tweet'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIaiuRviW5pS"
      },
      "source": [
        "Попробуйте использовать обычный счетчик слов для извлечения признаков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsr0wT4NW2LW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8TxxultNlOq"
      },
      "source": [
        "Точность в качестве метрики работает хорошо только на сбалансированных наборах данных, поэтому для оценки результатов работы  алгоритма будем использовать F1-метрику."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx29ZstgNXqb"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "\n",
        "    predictions = classifier.predict(feature_vector_valid)    \n",
        "\n",
        "    return metrics.f1_score(valid_y,predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YEjiEUDNrLj"
      },
      "source": [
        "Для начала обучим лог-регрессию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG3DifE-Nn-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c58e0af-b36e-40e3-cdca-e183b6924e2f"
      },
      "source": [
        "accuracyORIGINAL = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression Baseline, WordLevel TFIDF: \", accuracyORIGINAL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression Baseline, WordLevel TFIDF:  0.5307189542483659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0baBnaYN7a0"
      },
      "source": [
        "Как видно, результат оставляет желать лучшего.\n",
        "\n",
        "Что можно сделать с данными?\n",
        "\n",
        "Было бы неплохо как-то увеличить  количество негативных примеров, или же уменьшить количество положительных. Для этого существуют различные техники аугментации данных. \n",
        "В Python для этих целей есть библиотека imblearn (imbalanced-learn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6zNqmnUN1WB"
      },
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                    NearMiss, \n",
        "                                    InstanceHardnessThreshold,\n",
        "                                    CondensedNearestNeighbour,\n",
        "                                    EditedNearestNeighbours,\n",
        "                                    RepeatedEditedNearestNeighbours,\n",
        "                                    AllKNN,\n",
        "                                    NeighbourhoodCleaningRule,\n",
        "                                    OneSidedSelection,\n",
        "                                    TomekLinks)\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efoIi6euYMQs"
      },
      "source": [
        "Итак, в качестве инструментов для аугментации рассмотрим: under-sampling, over-sampling и их комбинацию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqhD2dodYwJl"
      },
      "source": [
        "**Under-sampling** уравновешивает данные за счет уменьшения размера  превалирующего класса.\n",
        "Этот метод разумно использовать, когда количество данных достаточно велико, иначе есть риск остаться и вовсе без обучающих примеров.\n",
        "\n",
        "Итак, логика действия довольно проста: мы просто случайным образом убираем лишние экземпляры из превалирующего класса.\n",
        "\n",
        "Так как в нашем примере лишь 7% всех твитов имеют негативную окраску, уравновешивание позитивного набора с этими 7-ю процентами вряд ли обеспечит хороший результат.\n",
        "\n",
        "Попробуем..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQVfIvf0aTrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072c11dd-2bce-4505-e66e-12ed4e860df5"
      },
      "source": [
        "rus = RandomUnderSampler(random_state=0, replacement=True)\n",
        "rus_xtrain_tfidf, rus_train_y = rus.fit_resample(xtrain_tfidf, train_y)\n",
        "accuracyrus = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),rus_xtrain_tfidf, rus_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression RUS, WordLevel TFIDF: \", accuracyrus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression RUS, WordLevel TFIDF:  0.5039637599093997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnwqxfGeaa9R"
      },
      "source": [
        "Действительно, все стало только хуже.\n",
        "\n",
        "Попробуем другие алгоритмы **under-sampling**.\n",
        "\n",
        "Например, **NearMiss**. Данный алгоритм выбирает, какие экземпляры нужно оставить в превалирующем классе на основании некоторых эвристик. Существует три варианта данного алгоритма:\n",
        "\n",
        "**NearMiss-1** оставляет те экземпляры из превалирующего класса, для которых среднее расстояние до *k* ближайших соседей из миноритарного класса будет наименьшим.\n",
        "\n",
        "**NearMiss-2** оставляет те экземпляры из превалирующего класса, для которых среднее расстояние до *k* самых дальних соседей из миноритарного класса будет наименьшим.\n",
        "\n",
        "**NearMiss-3** состоит из двух шагов: сначала, для каждого экземпляра из миноритарного класса выбирается *k* ближайших соседей из превалирующего класса, затем из большего класса выбираются те экземпляры, для которых среднее расстояние до *k* ближайших соседей максимальное."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSlgmHmOd9tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7688e599-b03a-4f5b-af3f-a2c360fcecea"
      },
      "source": [
        "for sampler in (NearMiss(version=1),NearMiss(version=2),NearMiss(version=3)):\n",
        "    nm_xtrain_tfidf, nm_train_y = sampler.fit_resample(xtrain_tfidf, train_y)\n",
        "    accuracysm = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)\n",
        "    print (\"Logistic regression NearMiss(version= {0}), WordLevel TFIDF: \".format(sampler.version), accuracysm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression NearMiss(version= 1), WordLevel TFIDF:  0.2599945607832472\n",
            "Logistic regression NearMiss(version= 2), WordLevel TFIDF:  0.4953219592735278\n",
            "Logistic regression NearMiss(version= 3), WordLevel TFIDF:  0.5402965828497743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9nNg2irexcc"
      },
      "source": [
        "**Edited Nearest Neighbor (ENN)**\n",
        "\n",
        "ENN удаляет из большего класса элемент, если класс его ближайшего соседа отличается от его собственного."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFjd6PzjfWEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b31107-e86c-4238-8293-df810d07c9a3"
      },
      "source": [
        "enn_xtrain_tfidf, enn_train_y = EditedNearestNeighbours().fit_resample(xtrain_tfidf, train_y)\n",
        "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),enn_xtrain_tfidf, enn_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression {0}, WordLevel TFIDF: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression {0}, WordLevel TFIDF:  0.5435897435897437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VcHHV4jgZ50"
      },
      "source": [
        "Как вы поняли, при применении **Under-sampling** техник новые данные не генерируются, в отличие от **Over-sampling**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHYj7jW7z5q"
      },
      "source": [
        "# Over-sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODhcR4G672lq"
      },
      "source": [
        "Итак, когда данных недостаточно или количество экземпляров в миноритарном классе очень мало, применяется **Over-sampling**. \n",
        "\n",
        "При применении этой техники балансировка данных происходит за счет увеличения количества экземпляров в миноритарном классе. Новые элементы генерируются за счет: повторения, бутстрэппинга, SMOTE (Synthetic Minority Over-Sampling Technique) или ADASYN (Adaptive synthetic sampling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea9aYY4mxsqo"
      },
      "source": [
        "**Random Over-sampling**: случайным образом дублируются некоторые элементы из миноритарного класса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-6Ivm0ZOCOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4c87df-496d-41b2-9f1b-46607c14ea37"
      },
      "source": [
        "#Random Over Sampling\n",
        "ros = RandomOverSampler(random_state=777)\n",
        "ros_xtrain_tfidf, ros_train_y = ros.fit_resample(xtrain_tfidf, train_y)\n",
        "accuracyROS = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression ROS, WordLevel TFIDF: \", accuracyROS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression ROS, WordLevel TFIDF:  0.6746575342465755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXp1gpdDz5p5"
      },
      "source": [
        "**SMOTE Over-sampling**\n",
        "\n",
        "Алгоритм SMOTE основан на идее генерации некоторого количества искусственных примеров, которые были бы «похожи» на имеющиеся в миноритарном классе, но при этом не дублировали их.\n",
        "\n",
        "Для создания новой записи находят разность $d=X_b-X_a,$ где $ X_b, X_a -$ векторы признаков «соседних» примеров $a$ и $b$ из миноритарного класса. \n",
        "\n",
        "Их находят, используя алгоритм ближайшего соседа (*KNN*). В данном случае необходимо и достаточно для примера $b$ получить набор из $k$ соседей, из которого в дальнейшем будет выбрана запись $b$. Остальные шаги алгоритма *KNN* не требуются.\n",
        "\n",
        "Далее из $d$ путем умножения каждого его элемента на случайное число в интервале (0, 1) получают $\\hat{d}$. Вектор признаков нового примера вычисляется путем сложения $X_a$ и $\\hat{d}$. \n",
        "\n",
        "Алгоритм **SMOTE** позволяет задавать количество записей, которое необходимо искусственно сгенерировать. Степень сходства примеров $a$ и $b$ можно регулировать путем изменения значения $k$ (числа ближайших соседей)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWzqbIWsOEg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3377c1-1a46-40cf-c48b-11d99dc82872"
      },
      "source": [
        "sm = SMOTE(random_state=777)\n",
        "sm_xtrain_tfidf, sm_train_y = sm.fit_resample(xtrain_tfidf, train_y)\n",
        "accuracySMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression SMOTE, WordLevel TFIDF: \", accuracySMOTE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression SMOTE, WordLevel TFIDF:  0.6684256816182937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gpJkvdb10Sg"
      },
      "source": [
        "Итак, по сравнению с **Random Over-sampling** разница небольшая.\n",
        "\n",
        "Проверьте результаты **Random Over-sampling** и **SMOTE Over-sampling** для реальных тестовых данных (*test_clean*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V-VoCAO7qw0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jz0o7k82YoX"
      },
      "source": [
        "Следующий алгоритм **ASMO: Adaptive synthetic minority oversampling**.\n",
        "\n",
        "\n",
        "\n",
        "Сгенерировать искусственные записи в пределах отдельных кластеров на основе всех классов. Для каждого примера миноритарного класса находят m ближайших соседей, и на основе них (также как в SMOTE) создаются новые записи.\n",
        "\n",
        "1.   Если для каждого $i$-ого примера миноритарного класса из $k$ ближайших соседей $g$ ($g\\leq k$) принадлежит к мажоритарному, то набор данных считается «рассеянным». В этом случае используют алгоритм **ASMO**, иначе применяют **SMOTE** (как правило, $g$ задают равным 20).\n",
        "2.   Используя только примеры миноритарного класса, выделить несколько кластеров (например, алгоритмом $k$-means).\n",
        "3.   Сгенерировать искусственные записи в пределах отдельных кластеров на основе всех классов. Для каждого примера миноритарного класса находят m ближайших соседей, и на основе них (также как в **SMOTE**) создаются новые записи.\n",
        "\n",
        "Такая модификация алгоритма **SMOTE** делает его более адаптивным к различным наборам данных с несбалансированными классами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mClRDnFM1weo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6414af63-8fc8-4421-97fa-938988bef58d"
      },
      "source": [
        "ad = ADASYN(random_state=777)\n",
        "ad_xtrain_tfidf, ad_train_y = ad.fit_resample(xtrain_tfidf, train_y)\n",
        "accuracyADASYN = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ad_xtrain_tfidf, ad_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression ADASYN, WordLevel TFIDF: \", accuracyADASYN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression ADASYN, WordLevel TFIDF:  0.6554054054054054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHRcvIRP4fx7"
      },
      "source": [
        "И опять проверим на реальных тестовых примерах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzMbjvnRyFD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br9FjH077ix0"
      },
      "source": [
        "# Комбинация **Under-** и **Over-sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-RCI42f42Hi"
      },
      "source": [
        "В *imblearn* реализованы две возможные комбинации:\n",
        "\n",
        "\n",
        "1.   **SMOTE** + **ENN**\n",
        "2.   **SMOTE** + **Tomek Link Removal** (Пара двух ближайших соседей, которые принадлежат разным классам называется *Tomek link*. Under-sampling заключается в удалении всех таких элементов из мажоритарного класса)\n",
        "\n",
        "Подробнее: https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.combine\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pg1YBvT4-xP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de054649-48bd-46dc-fd73-52dd25b29cc9"
      },
      "source": [
        "se = SMOTEENN(random_state=42)\n",
        "se_xtrain_tfidf, se_train_y = se.fit_resample(xtrain_tfidf, train_y)\n",
        "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),se_xtrain_tfidf, se_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression SMOTEENN: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression SMOTEENN:  0.37485626676887696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gd9dCEd5aDs"
      },
      "source": [
        "Первый метод сработал плохо. Оцените работу второго подхода."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkZofkXo5RG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "a95a143f-d241-4d82-bc52-18cab5eb6baa"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic regression SMOTEENN:  0.6945337620578779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}